{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrJGRDTS_SxP",
        "outputId": "c3ccddff-d41f-454e-91b9-63c8e992807b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully.\n",
            "Files in extracted folder:\n",
            "['332_AB.jpg', '689_AB.jpg', '797_AB.jpg', '841_AB.jpg', '440_AB.jpg', '1144_AB.jpg', '520_AB.jpg', '772_AB.jpg', '785_AB.jpg', '849_AB.jpg']\n",
            "Paired and unpaired data loaders are ready.\n",
            "Number of paired samples: 400\n",
            "Number of unpaired samples: 400\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# Paths\n",
        "zip_path = \"/content/edge2shoes.zip\"  # The path to your zip file on Google Colab\n",
        "extract_path = \"/content/extracted_edge2shoes/\"  # Directory to extract the dataset\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully.\")\n",
        "\n",
        "# Check if the directory exists and list some files\n",
        "if os.path.exists(extract_path):\n",
        "    print(\"Files in extracted folder:\")\n",
        "    print(os.listdir(extract_path)[:10])  # Display the first 10 files or folders\n",
        "else:\n",
        "    print(\"Extraction path does not exist.\")\n",
        "\n",
        "# Define transformations for the dataset images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),     # Resize to 256x256\n",
        "    transforms.ToTensor(),             # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Paired Dataset Class\n",
        "class PairedEdge2ShoesDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory path with images (each containing paired information).\n",
        "            transform (callable, optional): Transform to be applied on both images.\n",
        "        \"\"\"\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(data_dir, \"*.jpg\")))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the paired image\n",
        "        paired_image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "\n",
        "        # Split the image into two halves (assuming the left half is edge and right half is the real image)\n",
        "        width, height = paired_image.size\n",
        "        edge_image = paired_image.crop((0, 0, width // 2, height))\n",
        "        real_image = paired_image.crop((width // 2, 0, width, height))\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            edge_image = self.transform(edge_image)\n",
        "            real_image = self.transform(real_image)\n",
        "\n",
        "        return edge_image, real_image\n",
        "\n",
        "# Unpaired Dataset Class (edge and real images loaded independentl\n",
        "class UnpairedEdge2ShoesDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory path with images.\n",
        "            transform (callable, optional): Transform to be applied on images.\n",
        "        \"\"\"\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(data_dir, \"*.jpg\")))\n",
        "        self.edge_paths = self.image_paths.copy()  # For edge images\n",
        "        self.real_paths = self.image_paths.copy()  # For real images\n",
        "        self.transform = transform\n",
        "\n",
        "        # Shuffle the paths for unpaired loading\n",
        "        random.shuffle(self.edge_paths)\n",
        "        random.shuffle(self.real_paths)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.edge_paths), len(self.real_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load unpaired edge image\n",
        "        edge_image_full = Image.open(self.edge_paths[idx]).convert(\"RGB\")\n",
        "        width, height = edge_image_full.size\n",
        "        edge_image = edge_image_full.crop((0, 0, width // 2, height))\n",
        "\n",
        "        # Load unpaired real image\n",
        "        real_image_full = Image.open(self.real_paths[idx]).convert(\"RGB\")\n",
        "        real_image = real_image_full.crop((width // 2, 0, width, height))\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            edge_image = self.transform(edge_image)\n",
        "            real_image = self.transform(real_image)\n",
        "\n",
        "        return edge_image, real_image\n",
        "\n",
        "# Create paired and unpaired datasets\n",
        "paired_dataset = PairedEdge2ShoesDataset(data_dir=extract_path, transform=transform)\n",
        "unpaired_dataset = UnpairedEdge2ShoesDataset(data_dir=extract_path, transform=transform)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Define DataLoader with increased batch size if GPU memory permits\n",
        "paired_loader = DataLoader(paired_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "unpaired_loader = DataLoader(unpaired_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Paired and unpaired data loaders are ready.\")\n",
        "print(\"Number of paired samples:\", len(paired_dataset))\n",
        "print(\"Number of unpaired samples:\", len(unpaired_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HybridUNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_dropout=False):\n",
        "        super(HybridUNetGenerator, self).__init__()\n",
        "\n",
        "        # Use InstanceNorm instead of BatchNorm\n",
        "        norm_layer = nn.InstanceNorm2d\n",
        "\n",
        "        # Shared Encoder Layers\n",
        "        self.down1 = self.conv_block(in_channels, 64, norm_layer, use_dropout)\n",
        "        self.down2 = self.conv_block(64, 128, norm_layer, use_dropout)\n",
        "        self.down3 = self.conv_block(128, 256, norm_layer, use_dropout)\n",
        "        self.down4 = self.conv_block(256, 512, norm_layer, use_dropout)\n",
        "        self.down5 = self.conv_block(512, 512, norm_layer, use_dropout)\n",
        "        self.down6 = self.conv_block(512, 512, norm_layer, use_dropout)\n",
        "\n",
        "        # Decoder for cGAN (Edge-to-Image)\n",
        "        self.up1_cgan = self.upconv_block(512, 512, norm_layer)\n",
        "        self.up2_cgan = self.upconv_block(1024, 512, norm_layer)\n",
        "        self.up3_cgan = self.upconv_block(1024, 256, norm_layer)\n",
        "        self.up4_cgan = self.upconv_block(512, 128, norm_layer)\n",
        "        self.up5_cgan = self.upconv_block(256, 64, norm_layer)\n",
        "        self.final_cgan = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Decoder for CycleGAN (Cycle Consistency)\n",
        "        self.up1_cyclegan = self.upconv_block(512, 512, norm_layer)\n",
        "        self.up2_cyclegan = self.upconv_block(1024, 512, norm_layer)\n",
        "        self.up3_cyclegan = self.upconv_block(1024, 256, norm_layer)\n",
        "        self.up4_cyclegan = self.upconv_block(512, 128, norm_layer)\n",
        "        self.up5_cyclegan = self.upconv_block(256, 64, norm_layer)\n",
        "        self.final_cyclegan = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, norm_layer, use_dropout=False):\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            norm_layer(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "        if use_dropout:\n",
        "            layers.append(nn.Dropout(0.5))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels, norm_layer):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            norm_layer(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared Encoder forward pass with down-sampling\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "\n",
        "        # cGAN Decoder forward pass (Edge-to-Image)\n",
        "        u1_cgan = self.up1_cgan(d6)\n",
        "        u2_cgan = self.up2_cgan(torch.cat([u1_cgan, d5], dim=1))\n",
        "        u3_cgan = self.up3_cgan(torch.cat([u2_cgan, d4], dim=1))\n",
        "        u4_cgan = self.up4_cgan(torch.cat([u3_cgan, d3], dim=1))\n",
        "        u5_cgan = self.up5_cgan(torch.cat([u4_cgan, d2], dim=1))\n",
        "        output_cgan = self.final_cgan(torch.cat([u5_cgan, d1], dim=1))\n",
        "\n",
        "        # CycleGAN Decoder forward pass (Cycle Consistency)\n",
        "        u1_cyclegan = self.up1_cyclegan(d6)\n",
        "        u2_cyclegan = self.up2_cyclegan(torch.cat([u1_cyclegan, d5], dim=1))\n",
        "        u3_cyclegan = self.up3_cyclegan(torch.cat([u2_cyclegan, d4], dim=1))\n",
        "        u4_cyclegan = self.up4_cyclegan(torch.cat([u3_cyclegan, d3], dim=1))\n",
        "        u5_cyclegan = self.up5_cyclegan(torch.cat([u4_cyclegan, d2], dim=1))\n",
        "        output_cyclegan = self.final_cyclegan(torch.cat([u5_cyclegan, d1], dim=1))\n",
        "\n",
        "        return output_cgan, output_cyclegan\n",
        "\n",
        "\n",
        "# Discriminator stays the same as PatchGAN\n",
        "class PatchGANDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, use_dropout=False):\n",
        "        super(PatchGANDiscriminator, self).__init__()\n",
        "\n",
        "        # Use InstanceNorm instead of BatchNorm\n",
        "        norm_layer = nn.InstanceNorm2d\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            self.conv_block(64, 128, norm_layer, use_dropout),\n",
        "            self.conv_block(128, 256, norm_layer, use_dropout),\n",
        "            self.conv_block(256, 512, norm_layer, use_dropout),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, norm_layer, use_dropout=False):\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            norm_layer(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "        if use_dropout:\n",
        "            layers.append(nn.Dropout(0.5))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "# Loss Functions\n",
        "adversarial_loss = nn.MSELoss()  # for discriminator's feedback\n",
        "l1_loss = nn.L1Loss()  # for cycle consistency and paired data loss"
      ],
      "metadata": {
        "id": "oBYrdX61_YGE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "# Instantiate the hybrid generator with shared encoder and separate decoders for cGAN and CycleGAN\n",
        "gen_hybrid = HybridUNetGenerator(in_channels=3, out_channels=3)\n",
        "\n",
        "# Instantiate the discriminators (PatchGAN for both cGAN and CycleGAN)\n",
        "disc_cGAN = PatchGANDiscriminator(in_channels=3)\n",
        "disc_cycle = PatchGANDiscriminator(in_channels=3)\n",
        "\n",
        "# Weight initialization function (same as before)\n",
        "def weights_init(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        init.normal_(m.weight, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal_(m.weight, 1.0, 0.02)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "# Apply weight initialization to all models\n",
        "gen_hybrid.apply(weights_init)\n",
        "disc_cGAN.apply(weights_init)\n",
        "disc_cycle.apply(weights_init)\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gen_hybrid, disc_cGAN, disc_cycle = gen_hybrid.to(device), disc_cGAN.to(device), disc_cycle.to(device)\n",
        "\n",
        "# Define optimizers for both cGAN and CycleGAN\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "# Optimizers for the generators (cGAN and CycleGAN) and discriminators (PatchGAN)\n",
        "optimizer_g = optim.Adam(gen_hybrid.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizer_d_cGAN = optim.Adam(disc_cGAN.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizer_d_cycle = optim.Adam(disc_cycle.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "# Define learning rate schedulers for both the cGAN and CycleGAN components\n",
        "g_scheduler = optim.lr_scheduler.StepLR(optimizer_g, step_size=10, gamma=0.5)  # Same scheduler for both cGAN and CycleGAN\n",
        "d_scheduler_cGAN = optim.lr_scheduler.StepLR(optimizer_d_cGAN, step_size=10, gamma=0.5)\n",
        "d_scheduler_cycle = optim.lr_scheduler.StepLR(optimizer_d_cycle, step_size=10, gamma=0.5)"
      ],
      "metadata": {
        "id": "prgokeQh_fl9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.amp as amp\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set environment variable to handle memory fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize loss functions\n",
        "criterion_gan = nn.BCEWithLogitsLoss()\n",
        "criterion_l1 = nn.L1Loss()\n",
        "\n",
        "# Initialize the GradScaler for mixed precision\n",
        "scaler = amp.GradScaler(enabled=True)\n",
        "\n",
        "# Loss function wrappers\n",
        "def gan_loss(output, target, criterion):\n",
        "    return criterion(output, target)\n",
        "\n",
        "def cycle_consistency_loss(recovered, real, criterion, lambda_cycle=10):\n",
        "    return criterion(recovered, real) * lambda_cycle\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Initialize lists to store losses for plotting\n",
        "g_losses = []\n",
        "d_losses_cGAN = []\n",
        "d_losses_cycle = []\n",
        "\n",
        "# Create a directory for checkpoints\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    gen_hybrid.train()\n",
        "    disc_cGAN.train()\n",
        "    disc_cycle.train()\n",
        "\n",
        "    # Initialize epoch losses\n",
        "    g_loss_epoch = 0.0\n",
        "    d_loss_cGAN_epoch = 0.0\n",
        "    d_loss_cycle_epoch = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for i, (real_A, real_B) in enumerate(zip(paired_loader, unpaired_loader)):\n",
        "        real_A, real_B = real_A[0].to(device), real_B[0].to(device)\n",
        "\n",
        "        # ============================\n",
        "        # Update the Discriminators\n",
        "        # ============================\n",
        "\n",
        "        # Train Discriminator for cGAN\n",
        "        optimizer_d_cGAN.zero_grad()\n",
        "        with amp.autocast(device_type='cuda'):\n",
        "            fake_B_cgan, _ = gen_hybrid(real_A)\n",
        "            disc_real_B = disc_cGAN(real_B)\n",
        "            disc_fake_B = disc_cGAN(fake_B_cgan.detach())\n",
        "\n",
        "            # Discriminator cGAN Loss\n",
        "            d_loss_cGAN = (gan_loss(disc_real_B, torch.ones_like(disc_real_B).to(device), criterion_gan) +\n",
        "                           gan_loss(disc_fake_B, torch.zeros_like(disc_fake_B).to(device), criterion_gan)) / 2\n",
        "\n",
        "        # Backpropagation and optimization for discriminator cGAN\n",
        "        scaler.scale(d_loss_cGAN).backward()\n",
        "        scaler.step(optimizer_d_cGAN)\n",
        "        scaler.update()\n",
        "\n",
        "        # Train Discriminator for CycleGAN\n",
        "        optimizer_d_cycle.zero_grad()\n",
        "        with amp.autocast(device_type='cuda'):\n",
        "            _, fake_A = gen_hybrid(real_B)\n",
        "            disc_real_A = disc_cycle(real_A)\n",
        "            disc_fake_A = disc_cycle(fake_A.detach())\n",
        "\n",
        "            # Discriminator CycleGAN Loss\n",
        "            d_loss_cycle = (gan_loss(disc_real_A, torch.ones_like(disc_real_A).to(device), criterion_gan) +\n",
        "                            gan_loss(disc_fake_A, torch.zeros_like(disc_fake_A).to(device), criterion_gan)) / 2\n",
        "\n",
        "        # Backpropagation and optimization for discriminator CycleGAN\n",
        "        scaler.scale(d_loss_cycle).backward()\n",
        "        scaler.step(optimizer_d_cycle)\n",
        "        scaler.update()\n",
        "\n",
        "        # ============================\n",
        "        # Update the Generator\n",
        "        # ============================\n",
        "        optimizer_g.zero_grad()\n",
        "        with amp.autocast(device_type='cuda'):\n",
        "            # Generator cGAN Loss\n",
        "            g_loss_cGAN = gan_loss(disc_cGAN(fake_B_cgan), torch.ones_like(disc_real_B).to(device), criterion_gan)\n",
        "\n",
        "            # Cycle Consistency Loss\n",
        "            _, fake_A = gen_hybrid(real_B)\n",
        "            recovered_A, _ = gen_hybrid(fake_A)\n",
        "            cycle_loss = cycle_consistency_loss(recovered_A, real_A, criterion_l1)\n",
        "\n",
        "            # Total Generator Loss\n",
        "            g_loss = g_loss_cGAN + cycle_loss\n",
        "\n",
        "        # Backpropagation and optimization for generator\n",
        "        scaler.scale(g_loss).backward()\n",
        "        scaler.step(optimizer_g)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate epoch losses for averaging\n",
        "        g_loss_epoch += g_loss.item()\n",
        "        d_loss_cGAN_epoch += d_loss_cGAN.item()\n",
        "        d_loss_cycle_epoch += d_loss_cycle.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Print losses\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}], \"\n",
        "                  f\"g_loss: {g_loss.item():.4f}, \"\n",
        "                  f\"d_loss_cGAN: {d_loss_cGAN.item():.4f}, \"\n",
        "                  f\"d_loss_cycle: {d_loss_cycle.item():.4f}\")\n",
        "\n",
        "    # Calculate average losses for the epoch\n",
        "    g_losses.append(g_loss_epoch / num_batches)\n",
        "    d_losses_cGAN.append(d_loss_cGAN_epoch / num_batches)\n",
        "    d_losses_cycle.append(d_loss_cycle_epoch / num_batches)\n",
        "\n",
        "    # Update schedulers\n",
        "    g_scheduler.step()\n",
        "    d_scheduler_cGAN.step()\n",
        "    d_scheduler_cycle.step()\n",
        "\n",
        "    # Save checkpoints periodically\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save(gen_hybrid.state_dict(), f\"checkpoints/gen_hybrid_epoch_{epoch+1}.pth\")\n",
        "        torch.save(disc_cGAN.state_dict(), f\"checkpoints/disc_cGAN_epoch_{epoch+1}.pth\")\n",
        "        torch.save(disc_cycle.state_dict(), f\"checkpoints/disc_cycle_epoch_{epoch+1}.pth\")\n",
        "\n",
        "# Plot the Training Losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(g_losses, label='Generator Loss')\n",
        "plt.plot(d_losses_cGAN, label='Discriminator cGAN Loss')\n",
        "plt.plot(d_losses_cycle, label='Discriminator Cycle Loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpvAwpki_v7t",
        "outputId": "c6c9132e-e04d-4314-94a1-92256f31ca5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [0], g_loss: 10.4695, d_loss_cGAN: 0.7248, d_loss_cycle: 0.7309\n"
          ]
        }
      ]
    }
  ]
}